# run script

invisible(lapply(list.files("R", full.names = TRUE, recursive = TRUE), source))
library(stringr)
library(openxlsx)

site_name = "Bokspits_2"

#============ Data Prep =====================

x <- build_cube_5_CHM(site_name = "Bokspits_2")

# x <- rast("data_out/Bokspits_2/Bokspits_2_stack.tif") #access the intermediate layer

v <- build_ml_df(cube = x,
                 site_name = "Bokspits_2",
                 df_type="grid")

v <- "data_out/Bokspits_2/Bokspits_2ML_in_grid_level.rds"

#================ ML pipeline ================
# Auto ML ------------------

task <- build_task(v, site_name="Bokspits_2")

tune_spcv <- mlr3::rsmp("spcv_coords", folds = 20)

# #inspect the spatial cv
autoplot(tune_spcv, task=task, 1:3)


# ================ model tuning. =================
# xgboost first.
tic()
xgb.lrn <- lts('classif.xgboost.rbv2')$get_learner()

# here we can add the option for filtering based on importance as a hyper parameter.
xgb.lrn.filt <- po("filter", filter=flt("importance"), filter.frac=to_tune(0.1, 1))%>>%
  xgb.lrn 

xgb_tune <- tune_lrnr(
  .task = task,
  .lrnr = xgb.lrn.filt,
  .resamp = tune_spcv,
  .measure = msr("classif.acc"),
  .n_evals = 50,
#  sub.sample = 0.1,
  .tune_method = "random_search",
#.tune_method = "random_search",
  .test.scale = TRUE,
  .test.pca = TRUE,
#.test.pca = TRUE,
  .workers = future::availableCores() - 2,
  .verbose = FALSE,
  .seed = 5446
)

plot(xgb_tune$graph)
xgb_tune$tun_inst$tuning_result
xgb_tune$tun_inst$learner$model$importance$features # view selected features if filter is used.
xgb_tune$tun_inst$learner$model$importance$scores

toc()
# now for svm

tic()
svm.lrn <- lts('classif.svm.rbv2', type=to_tune("C-classification"))$get_learner()

svm_tune <- tune_lrnr(
  .task = task,
  .lrnr = svm.lrn,
  .resamp = tune_spcv,
  .measure = msr("classif.acc"),
#  sub.sample = 0.1,
  .n_evals = 50,
  .tune_method = "random_search",
#.tune_method = "random_search",
  .test.scale = TRUE,
#.test.pca = TRUE
  .test.pca = TRUE
)
plot(svm_tune$graph)
svm_tune$tun_inst$tuning_result

toc()

# ranger random forest.
tic()
rf.lrn <- lts('classif.ranger.rbv2')$get_learner()

rf.lrn.filt <- po("filter", filter=flt("importance"), 
                  filter.frac=to_tune(0.1, 1)) %>>%
  rf.lrn 

rf_tune <- tune_lrnr(
  .task = task,
  .lrnr = rf.lrn.filt,
  .resamp = tune_spcv,
  .measure = msr("classif.acc"),
#  sub.sample = 0.1,
  .n_evals = 50,
  .tune_method = "random_search",
  .test.scale = TRUE,
  .test.pca = TRUE
#.test.pca = TRUE

)

toc()

plot(rf_tune$graph)
rf_tune$tun_inst$tuning_result
rf_tune$tun_inst$learner$model$importance$scores
# ensemble example
tic()

ens.lrn.tune <- gunion(list(
  po("learner_cv", xgb.lrn, id = "ens_xgb"),
  po("learner_cv", svm.lrn, id = "ens_rf"),
  po("learner_cv", rf.lrn, id = "ens_svm"),
  po("nop", "ens_nop")
)) %>>%
  po("featureunion", id = "ens_union") %>>%
  lrn("classif.ranger", id = "master_rf")


ens_tune <- tune_lrnr(
  .task = task,
  .lrnr = ens.lrn.tune,
  .resamp = tune_spcv,
  .measure = msr("classif.acc"),
# sub.sample = 0.5,
  .n_evals = 50,
 # .tune_method = "random_search", 
.tune_method = "random_search", # definitely use "mbo" for this!
  .test.scale = TRUE,
  .test.pca = TRUE
)

# You can just remove the PO("filter"..) or copy it from the above xfboost filter bit 

plot(ens_tune$graph)
ens_tune$tun_inst$tuning_result

toc()


# =================== benchmarking =================================
tic()
bench.mark <- benchmark_lrnrs(xgb_tune$tun_inst$learner, 
                              svm_tune$tun_inst$learner, 
                              rf_tune$tun_inst$learner, 
                              ens_tune$tun_inst$learner, 
                             lrn("classif.ranger", id="ranger.untuned"), #untuned random forest
                             .task = task, 
                             .resamp=tune_spcv)

bench.mark$aggregate(c(msr("classif.ce"),msr("classif.acc")))
bench.mark$aggregate(msr("classif.acc"))

toc()

p2 <- autoplot(bench.mark, measure=msr("classif.acc"))+
  scale_x_discrete(label = function(x) str_wrap(str_trunc(x, 60), width = 30)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))

p2



# decide which is the best...

# Resampling ------------------------------
# now proper resampling...here I'm using svm as it looks the best just switch the learner as needed.
#resamps <- ml_resample(.task = task,
 #           .learner = svm_tune$tun_inst$learner)

resamps <- ml_resample(.task = task,
                       .learner =  ens_tune$tun_inst$learner)

resamps$aggregate(c(msr("classif.acc"),
                    msr("classif.ce"))) # get aggregate scores
res.preds <-
  resamps$predictions() |>
  purrr::map(function(x = .x) x$confusion) |> 
  Reduce("+", x = _)
print(res.preds)


p3 <- autoplot(resamps, measure=msr("classif.acc"))
p3
 
# Prediction ----------------------------------

# tic()
# mod.pred <- predict_terra(x, 
#                   mod= svm_tune$tun_inst$learner,
#                   site_name="Bokspits_2",
#                   .workers=1)
# toc()
# # quick n dirt map.
# par(mfrow=c(1,2))
# plot(mod.pred, col=rev(brewer.pal(6, name="Dark2")), axes=FALSE)
# terra::plotRGB(x[[c(3,2,1)]], scale=0.6, smooth=TRUE, axes=FALSE)
# par(mfrow=c(1,1))
# 

### Exporting Data ------
# Exporting confusion matrix
as.data.frame(res.preds)
write.xlsx(res.preds, paste0("data_out/Confusion/res.preds_svm_",site_name,"_5_CHM.xlsx"), rowNames=FALSE)

#Exporting benchmark results
bench <-bench.mark$aggregate(msr("classif.acc"))
r <- bench$classif.acc
s <- bench$learner_id
b<- dplyr::bind_cols(r,s)
names(b) <- c('classif.acc','learner')

write.xlsx(b, paste0("data_out/Bench/MLR_bench_results_",site_name,"_5_CHM.xlsx"), rowNames=FALSE)


#Exporting benchmark plot
ggsave(
  p2,
  filename = paste0("data_out/Figures/benchmark_",site_name,"_model_classif_ac_5_CHM.png"),
  width =16,
  height = 8,
  units = "cm"
)
#Exporting final classification accuracy
ggsave(
  p3,
  filename = paste0("data_out/Figures/Final_",site_name,"_model_classif_ac_5_CHM.png"),
  width =16,
  height = 8,
  units = "cm"
)


### Exporting Data ------
### Exporting Data ------
# # Exporting confusion matrix
# as.data.frame(res.preds)
# write.xlsx(res.preds, paste0("data_out/Confusion/res.preds_ens_",site_name,".xlsx"), rowNames=FALSE)
# 
# #Exporting benchmark results
# bench <-bench.mark$aggregate(msr("classif.acc"))
# r <- bench$classif.acc
# s <- bench$learner_id
# b<- dplyr::bind_cols(r,s)
# names(b) <- c('classif.acc','learner')
# 
# write.xlsx(b, paste0("data_out/Bench/MLR_bench_results_",site_name,"_ens.xlsx"), rowNames=FALSE)
# 
# 
# #Exporting benchmark plot
# ggsave(
#   p2,
#   filename = paste0("data_out/Figures/benchmark_",site_name,"_model_classif_ac_ens.png"),
#   width =16,
#   height = 8,
#   units = "cm"
# )
# #Exporting final classification accuracy
# ggsave(
#   p3,
#   filename = paste0("data_out/Figures/Final_",site_name,"_model_classif_ac_ens.png"),
#   width =16,
#   height = 8,
#   units = "cm"
# )